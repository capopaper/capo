{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef46bd6",
   "metadata": {},
   "source": [
    "# CaPo: Evaluating Large Language Models for Strategic Knowledge Extraction in Capability-Based Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4d956b",
   "metadata": {},
   "source": [
    "## Extraction with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1763a",
   "metadata": {},
   "source": [
    "#### Environment and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694be89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\kolkhc\\Coding\\CaPo\\git_code\\capo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except Exception:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "def get_project_root() -> Path:\n",
    "    \"\"\"Finds the project's root directory by looking for a known file/folder.\"\"\"\n",
    "    p = Path.cwd()\n",
    "    while p != p.parent:\n",
    "        if (p / 'scripts').exists() or (p / '.git').exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return Path.cwd() # Fallback\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "\n",
    "dotenv_path = PROJECT_ROOT / \".env\"\n",
    "if dotenv_path.exists():\n",
    "    load_dotenv(dotenv_path)\n",
    "else:\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bc9ab",
   "metadata": {},
   "source": [
    "#### Project modules imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33fa19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.extractor import LLMExtractor\n",
    "from schemas.capability import Capabilities\n",
    "from utils.formatter import build_document_annotation, append_document_annotation\n",
    "from utils.io import save_json_to_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758ef90",
   "metadata": {},
   "source": [
    "#### Config (models, providers, files to analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8dfe6",
   "metadata": {},
   "source": [
    "We tried with different models. You can add/modify the `Model` and `ModelName` classes with your own models to try. We used models from two providers: Azure and Ollama. The files to analyze should be saved in a folder called `/data` located at the same level as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e65e9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(str, Enum):\n",
    "    GPT_4_1 = \"gpt-4.1\"\n",
    "    GPT_4_1_mini = \"gpt-4.1-mini\"\n",
    "    GPT_5_mini = \"gpt-5-mini\"\n",
    "    GEMMA_3 = \"hf.co/bartowski/google_gemma-3-27b-it-GGUF:Q8_0\"\n",
    "    MISTRAL_SMALL = (\n",
    "        \"hf.co/bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q8_0\"\n",
    "    )\n",
    "    QWEN_3 = \"hf.co/unsloth/Qwen3-4B-Instruct-2507-GGUF:Q8_0\"\n",
    "\n",
    "\n",
    "class ModelName(str, Enum):\n",
    "    GPT_4_1 = \"gpt-4.1\"\n",
    "    GPT_4_1_mini = \"gpt-4.1-mini\"\n",
    "    GPT_5_mini = \"gpt-5-mini\"\n",
    "    GEMMA_3 = \"gemma-3-27b\"\n",
    "    MISTRAL_SMALL = \"mistral-small\"\n",
    "    QWEN_3 = \"qwen3-4b\"\n",
    "\n",
    "\n",
    "class Provider(str, Enum):\n",
    "    AZURE = \"azure\"\n",
    "    OLLAMA = \"ollama\"\n",
    "\n",
    "\n",
    "# Choose defaults (you can tweak interactively):\n",
    "model_name = ModelName.GPT_5_mini\n",
    "model = Model.GPT_5_mini\n",
    "provider = Provider.AZURE\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\" / \"llm_annotations\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "filenames = [\n",
    "    \"document_1_capiciteitsdruk\",\n",
    "    \"document_2_cybercrime\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a5fc5",
   "metadata": {},
   "source": [
    "#### Load capabilities JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc7a6c0",
   "metadata": {},
   "source": [
    "The capabilities should be saved in the `/data` folder at the same level as this notebook. The name should be `capabilities.json` and the structure is:\n",
    "```json\n",
    "{\n",
    "  \"capabilities\": [\n",
    "    {\n",
    "      \"name\": \"<Capability_Name>\",\n",
    "      \"definition\": \"<Capability_Definition>\",\n",
    "      \"category\": \"<Capability_Category>\"\n",
    "    },\n",
    "  ],\n",
    "}\n",
    "```\n",
    "For more information see the capability schema in `/schemas/capability.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db4fd47",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Capabilities\ncapabilities\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(capabilities_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     capabilities_json = json.load(f)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m capabilities = \u001b[43mCapabilities\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapabilities_json\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\main.py:703\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, from_attributes, context, by_alias, by_name)\u001b[39m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    699\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    700\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    701\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for Capabilities\ncapabilities\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"
     ]
    }
   ],
   "source": [
    "capabilities_path = DATA_DIR / \"capabilities.json\"\n",
    "with open(capabilities_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    capabilities_json = json.load(f)\n",
    "\n",
    "capabilities = Capabilities.model_validate(capabilities_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4288fa7",
   "metadata": {},
   "source": [
    "#### Core extraction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c468c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = None\n",
    "run_input_tokens = 0\n",
    "run_output_tokens = 0\n",
    "\n",
    "for filename in tqdm(filenames, desc=\"Annotating\", leave=False):\n",
    "    print(f\"\\nAnnotating {filename}...\")\n",
    "    data_path = DATA_DIR / f\"{filename}.txt\"\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        document = f.read()\n",
    "\n",
    "    extractor = LLMExtractor(model=model, provider=provider)\n",
    "\n",
    "    # 1) Strategies\n",
    "    strategies = extractor.extract_strategies(document=document, temperature=0.2)\n",
    "    # 2) Goals\n",
    "    goals = extractor.extract_goals(\n",
    "        document=document, temperature=0.1, strategies=strategies\n",
    "    )\n",
    "    # 3) Trends\n",
    "    trends = extractor.extract_trends(\n",
    "        document=document, temperature=0.1, strategies=strategies, goals=goals\n",
    "    )\n",
    "    # 4) Grouping\n",
    "    grouped_strategies = extractor.group_strategies(\n",
    "        document=document, temperature=0.2, strategies=strategies\n",
    "    )\n",
    "    grouped_goals = extractor.group_goals(\n",
    "        document=document, temperature=0.2, goals=goals\n",
    "    )\n",
    "    grouped_trends = extractor.group_trends(\n",
    "        document=document, temperature=0.2, trends=trends\n",
    "    )\n",
    "    # 5) Strategy ↔ Goal links\n",
    "    strategy_goal_relations = extractor.link_strategies_and_goals(\n",
    "        document=document, temperature=0.2, strategies=strategies, goals=goals\n",
    "    )\n",
    "    # 6) Strategy ↔ Trend links\n",
    "    strategy_trend_relations = extractor.link_strategies_and_trends(\n",
    "        document=document, temperature=0.2, strategies=strategies, trends=trends\n",
    "    )\n",
    "    # 7) Strategy ↔ Capability links\n",
    "    strategy_capability_relations = extractor.link_strategies_and_capabilities(\n",
    "        document=document,\n",
    "        temperature=0.2,\n",
    "        strategies=strategies,\n",
    "        capabilities=capabilities,\n",
    "    )\n",
    "\n",
    "    run_input_tokens += getattr(extractor, \"input_tokens_total\", 0)\n",
    "    run_output_tokens += getattr(extractor, \"output_tokens_total\", 0)\n",
    "\n",
    "    document_annotation = build_document_annotation(\n",
    "        document_id=filename,\n",
    "        strategies=strategies,\n",
    "        goals=goals,\n",
    "        trends=trends,\n",
    "        grouped_strategies=grouped_strategies,\n",
    "        grouped_goals=grouped_goals,\n",
    "        grouped_trends=grouped_trends,\n",
    "        strategy_goal_relations=strategy_goal_relations,\n",
    "        strategy_trend_relations=strategy_trend_relations,\n",
    "        strategy_capability_relations=strategy_capability_relations,\n",
    "    )\n",
    "\n",
    "    bundle = append_document_annotation(\n",
    "        bundle=bundle,\n",
    "        annotator_id=f\"{provider}_{model_name}\",\n",
    "        document_annotation=document_annotation,\n",
    "    )\n",
    "\n",
    "print(\"\\nRun complete.\")\n",
    "print(f\"TOTAL input tokens:  {run_input_tokens}\")\n",
    "print(f\"TOTAL output tokens: {run_output_tokens}\")\n",
    "print(f\"TOTAL tokens:        {run_input_tokens + run_output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6247456",
   "metadata": {},
   "source": [
    "#### Save output to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e36871",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_json = json.dumps(bundle, indent=2, ensure_ascii=False)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_json_to_output(\n",
    "    filename=f\"{provider}_{model_name}\",\n",
    "    json_str=result_json,\n",
    "    foldername=\"output\",\n",
    ")\n",
    "\n",
    "print(f\"Saved to: {OUTPUT_DIR / (str(provider) + '_' + str(model_name) + '.json')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
